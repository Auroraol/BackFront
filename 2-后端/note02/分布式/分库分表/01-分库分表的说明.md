# 分库分表背景

## 引言

当一张表的数据达到500万（阿里的分库分表标准），或者是单表的数据容量达到了2GB 时，你查询一次所花的时间会变多，如果有联合查询的话，我想有可能会死在那儿了。分表的目的就在于此，减小数据库的负担，缩短查询时间。

分库分表是在海量数据下，由于单库、表数据量过大，导致数据库性能持续下降的问题，演变出的技术方案。

分库分表是由`分库`和`分表`这两个独立概念组成的，只不过通常分库与分表的操作会同时进行，以至于我们习惯性的将它们合在一起叫做分库分表。

<img src="01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/image-20240229132139971.png" alt="image-20240229132139971" style="zoom: 50%;" />

通过一定的规则，将原本数据量大的数据库拆分成多个单独的数据库，将原本数据量大的表拆分成若干个数据表，使得单一的库、表性能达到最优的效果（响应速度快），以此提升整体数据库性能。

## 为什么需要分库？

### 容量

我们给数据库实例分配的磁盘容量是固定的，数据量持续的大幅增长，用不了多久单机的容量就会承载不了这么多数据，解决办法简单粗暴，加容量！

### 连接数

单机的容量可以随意扩展，但数据库的连接数却是有限的，在高并发场景下多个业务同时对一个数据库操作，很容易将连接数耗尽导致`too many connections`报错，导致后续数据库无法正常访问。

可以通过`max_connections`查看MySQL最大连接数。

```
show variables like '%max_connections%'
```

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640.png)

将原本单数据库按不同业务拆分成订单库、物流库、积分库等不仅可以有效分摊数据库读写压力，也提高了系统容错性。

## 为什么需要分表？

做过报表业务的同学应该都体验过，一条SQL执行时间超过几十秒的场景。

导致数据库查询慢的原因有很多，SQL没命中索引、like扫全表、用了函数计算，这些都可以通过优化手段解决，可唯独数据量大是MySQL无法通过自身优化解决的。慢的根本原因是`InnoDB`存储引擎，聚簇索引结构的 B+tree 层级变高，磁盘IO变多查询性能变慢。

阿里的开发手册中有条建议，单表行数超500万行或者单表容量超过2GB，就推荐分库分表，然而理想和实现总是有差距的，阿里这种体量的公司不差钱当然可以这么用，实际上很多公司单表数据几千万、亿级别仍然不选择分库分表。

# 什么时候分库分表

**是否分库分表的关键指标是数据量**

以`fire100.top`这个网站的资源表 `t_resource`为例，系统在运行初始的时候，每天只有可怜的几十个资源上传，这时使用单库、单表的方式足以支持系统的存储，数据量小几乎没什么数据库性能瓶颈。

但某天开始一股神秘的流量进入，系统每日产生的资源数据量暴增至十万甚至上百万级别，这时资源表数据量到达千万级，查询响应变得缓慢，数据库的性能瓶颈逐渐显现。

以MySQL数据库为例，单表的数据量在达到亿条级别，通过加索引、SQL调优等传统优化策略，性能提升依旧微乎其微时，就可以考虑做分库分表了。

MySQL存储海量数据时会出现性能瓶颈，可以考虑用其他方案替代它, 比如高性能的非关系型数据库`MongoDB`(现在互联网上大部分公司的核心数据几乎是存储在关系型数据库（MySQL、Oracle等），而评论、点赞这些非核心数据还是可以考虑用`MongoDB`的。)

# 分库分表方法

分库与分表可以从：垂直（纵向）和 水平（横向）两种纬度进行拆分。下边我们以经典的订单业务举例，看看如何拆分。

![image-20240229133059083](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/image-20240229133059083.png)

### 垂直拆分

#### 1、垂直分库

垂直分库一般来说按照业务和功能的维度进行拆分，将不同业务数据分别放到不同的数据库中，核心理念 **`专库专用`**。

按业务类型对数据分离，剥离为多个数据库，像订单、支付、会员、积分相关等表放在对应的订单库、支付库、会员库、积分库。不同业务禁止跨库直连，获取对方业务数据一律通过`API`接口交互，这也是微服务拆分的一个重要依据。

**垂直分库很大程度上取决于业务的划分**，但有时候业务间的划分并不是那么清晰，比如：电商中订单数据的拆分，其他很多业务都依赖于订单数据，有时候界线不是很好划分。

垂直分库把一个库的压力分摊到多个库，提升了一些数据库性能，但并没有解决由于单表数据量过大导致的性能问题，所以就需要配合后边的分表来解决。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-17091846838964.png)

#### 2、垂直分表

垂直分表针对业务上字段比较多的大表进行的，一般是把业务宽表中比较独立的字段，或者不常用的字段拆分到单独的数据表中，是一种大表拆小表的模式。

例如：一张`t_order`订单表上有几十个字段，其中订单金额相关字段计算频繁，为了不影响订单表`t_order`的性能，就可以把订单金额相关字段拆出来单独维护一个`t_order_price_expansion`扩展表，这样每张表只存储原表的一部分字段，通过订单号`order_no`做关联，再将拆分出来的表路由到不同的库中。

数据库它是以行为单位将数据加载到内存中，这样拆分以后核心表大多是访问频率较高的字段，而且字段长度也都较短，因而可以加载更多数据到内存中，减少磁盘IO，增加索引查询的命中率，进一步提升数据库性能。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-17091846838975.png)

### 水平拆分

垂直分库、垂直分表后还是会存在单库、表数据量过大的问题，当我们的应用已经无法在细粒度的垂直切分时，依旧存在单库读写、存储性能瓶颈，这时就要配合水平分库、水平分表。

#### 1、水平分库

水平分库是**把同一个表按一定规则拆分**到不同的数据库中，每个库可以位于不同的服务器上，以此实现水平扩展，是一种常见的提升数据库性能的方式。

例如：`db_orde_1`、`db_order_2`两个数据库内有完全相同的`t_order`表，我们在访问某一笔订单时可以通过对订单的订单编号取模的方式 `订单编号 mod 2 （数据库实例数）` ，指定该订单应该在哪个数据库中操作。

这种方案往往能解决单库存储量及性能瓶颈问题，但由于同一个表被分配在不同的数据库中，数据的访问需要额外的路由工作，因此系统的复杂度也被提升了。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-17091846838986.png)

#### 2、水平分表

水平分表是在**同一个数据库内**，把一张大数据量的表按一定规则，**切分成多个结构完全相同表**，**而每个表只存原表的一部分数据。**

例如：一张`t_order`订单表有900万数据，经过水平拆分出来三个表，`t_order_1`、`t_order_2`、`t_order_3`，每张表存有数据300万，以此类推。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-17091846838987.png)

注意:  水平分表尽管拆分了表，但子表都还是在同一个数据库实例中，只是解决了单一表数据量过大的问题，并没有将拆分后的表分散到不同的机器上，还在竞争同一个物理机的CPU、内存、网络IO等。要想进一步提升性能，就需要将拆分后的表分散到不同的数据库中，达到分布式的效果。下图就是进行水平分库和分表:

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-17091846838988.png)

# 数据存放哪个库的表

> 分库分表以后会出现一个问题，一张表会出现在多个数据库里，到底该往哪个库的哪个表里存?

上边我们多次提到过`一定规则` ，其实这个规则它是一种路由算法，决定了一条数据具体应该存在哪个数据库的哪张表里。

常见的有 `取模算法` 、`范围限定算法`、`范围+取模算法` 、`预定义算法`

### 1、取模算法

关键字段取模（对hash结果取余数 hash(XXX) mod N)，N为数据库实例数或子表数量）是最为常见的一种路由方式。

以`t_order`订单表为例，先给数据库从 0 到 N-1进行编号，对 `t_order`订单表中`order_no`订单编号字段进行取模`hash(order_no) mod N`，得到余数`i`。`i=0`存第一个库，`i=1`存第二个库，`i=2`存第三个库，以此类推。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273314.png)

同一笔订单数据会落在同一个库、表里，查询时用相同的规则，用`t_order`订单编号作为查询条件，就能快速的定位到数据。

#### 优点

实现简单，数据分布相对比较均匀，不易出现请求都打到一个库上的情况。

#### 缺点

取模算法对集群的伸缩支持不太友好，集群中有N个数据库实`·hash(user_id) mod N`，当某一台机器宕机，本应该落在该数据库的请求就无法得到处理，这时宕掉的实例会被踢出集群。

此时机器数减少算法发生变化`hash(user_id) mod N-1`，同一用户数据落在了在不同数据库中，等这台机器恢复，用`user_id`作为条件查询用户数据就会少一部分。

### 2、范围限定算法

范围限定算法以某些范围字段，如`时间`或`ID区`拆分。

用户表`t_user`被拆分成`t_user_1`、`t_user_2`、`t_user_3`三张表，后续将`user_id`范围为1 ~ 1000w的用户数据放入`t_user_1`，1000~ 2000w放入`t_user_2`，2000~3000w放入`t_user_3`，以此类推。按日期范围划分同理。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273415.png)

#### 优点

- 单表数据量是可控的
- 水平扩展简单只需增加节点即可，无需对其他分片的数据进行迁移

#### 缺点

- 由于连续分片可能存在`数据热点`，比如按时间字段分片时，如果某一段时间（双11等大促）订单骤增，存11月数据的表可能会被频繁的读写，其他分片表存储的历史数据则很少被查询，导致数据倾斜，数据库压力分摊不均匀。

### 3、范围 + 取模算法(推荐)

这次我们先通过范围算法定义每个库的用户表`t_user`只存1000w数据，第一个`db_order_1`库存放`userId`从1 ~ 1000w，第二个库1000 ~ 2000w，第三个库2000~3000w，以此类推。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273416.png)

每个库里再把用户表`t_user`拆分成`t_user_1`、`t_user_2`、`t_user_3`等，对`userd`进行取模路由到对应的表中。

有效的避免数据分布不均匀的问题，数据库水平扩展也简单，直接添加实例无需迁移历史数据。

### 4、地理位置分片

地理位置分片其实是一个更大的范围，按城市或者地域划分，比如华东、华北数据放在不同的分片库、表。

### 5、预定义算法

预定义算法是事先已经明确知道分库和分表的数量，可以直接将某类数据路由到指定库或表中，查询的时候亦是如此。

# 分库分表出来的问题

相比于拆分前的单库单表，系统的数据存储架构演变到现在已经变得非常复杂。

1. 主键避重复问题
2. 公共表处理
3. 事务一致性
4. 跨节点关联
5. 跨节点分页，排序函数

## 分页、排序、跨节点联合查询

分页、排序、联合查询，这些看似普通，开发中使用频率较高的操作，在分库分表后却是让人非常头疼的问题。把分散在不同库中表的数据查询出来，再将所有结果进行汇总合并整理后提供给用户。

比如：我们要查询11、12月的订单数据，如果两个月的数据是分散到了不同的数据库实例，则要查询两个数据库相关的数据，在对数据合并排序、分页，过程繁琐复杂。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273417.png)

## 事务一致性

分库分表后由于表分布在不同库中，不可避免会带来跨库事务问题。后续会分别以阿里的`Seata`和MySQL的`XA`协议实现分布式事务，用来比较各自的优势与不足。

## 全局唯一的主键

分库分表后数据库表的主键ID业务意义就不大了，因为无法在标识唯一一条记录，例如：多张表`t_order_1`、`t_order_2`的主键ID全部从1开始会重复，此时我们需要主动为一条记录分配一个ID，这个全局唯一的ID就叫`分布式ID`，发放这个ID的系统通常被叫发号器。

## 多数据库高效治理

对多个数据库以及库内大量分片表的高效治理，是非常有必要，因为像某宝这种大厂一次大促下来，订单表可能会被拆分成成千上万个`t_order_n`表，如果没有高效的管理方案，手动建表、排查问题是一件很恐怖的事。

## 历史数据迁移

分库分表架构落地以后，首要的问题就是如何平滑的迁移历史数据，增量数据和全量数据迁移，这又是一个比较麻烦的事情，后边详细讲。

# 分库分表架构模式

分库分表架构主要有两种模式：`client`客户端模式和`proxy`代理模式

### 客户模式

`client`模式指分库分表的逻辑都在你的系统应用内部进行控制，应用会将拆分后的SQL直连多个数据库进行操作，然后本地进行数据的合并汇总等操作。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273418.png)

### 代理模式

`proxy`代理模式将应用程序与MySQL数据库隔离，业务方的应用不在需要直连数据库，而是连接proxy代理服务，代理服务实现了MySQL的协议，对业务方来说代理服务就是数据库，它会将SQL分发到具体的数据库进行执行，并返回结果。该服务内有分库分表的配置，根据配置自动创建分片表。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918584273419.png)

### 如何抉择

如何选择`client`模式和`proxy`模式，我们可以从以下几个方面来简单做下比较。

#### 1、性能

性能方面`client`模式表现的稍好一些，它是直接连接MySQL执行命令；`proxy`代理服务则将整个执行链路延长了，应用->代理服务->MySQL，可能导致性能有一些损耗，但两者差距并不是非常大。

#### 2、复杂度

`client`模式在开发使用通常引入一个jar可以；`proxy`代理模式则需要搭建单独的服务，有一定的维护成本，既然是服务那么就要考虑高可用，毕竟应用的所有SQL都要通过它转发至MySQL。

#### 3、升级

`client`模式分库分表一般是依赖基础架构团队的Jar包，一旦有版本升级或者Bug修改，所有应用到的项目都要跟着升级。小规模的团队服务少升级问题不大，如果是大公司服务规模大，且涉及到跨多部门，那么升级一次成本就比较高；

`proxy`模式在升级方面优势很明显，发布新功能或者修复Bug，只要重新部署代理服务集群即可，业务方是无感知的，但要保证发布过程中服务的可用性。

#### 4、治理、监控

`client`模式由于是内嵌在应用内，应用集群部署不太方便统一处理；`proxy`模式在对SQL限流、读写权限控制、监控、告警等服务治理方面更优雅一些。

# 分库分表的 21 条法则

我们结合具体业务场景，以`t_order`表为例进行架构优化。由于数据量已经达到亿级别，查询性能严重下降，因此我们采用了分库分表技术来处理这个问题。具体而言，我们将原本的单库分成了两个库，分别为`DB_1`和`DB_2`，并在每个库中再次进行分表处理，生成`t_order_1`和`t_order_2`两张表，实现对订单表的分库分表处理。使用水平分库分表:

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397326.png)

## 数据分片

通常我们在提到分库分表的时候，大多是以水平切分模式（水平分库、分表）为基础来说的，数据分片它将原本一张数据量较大的表 `t_order` 拆分生成数个**表结构完全一致**的小数据量表（拆分表） `t_order_0`、`t_order_1`、···、`t_order_n`，每张表只存储原大表中的一部分数据。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397427.png)

## 数据节点

数据节点是数据分片中一个不可再分的最小单元（表），它由数据源名称和数据表组成，例如上图中 `DB_1.t_order_1`、`DB_2.t_order_2` 就表示一个数据节点。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397428.png)

## 逻辑表

逻辑表是指具有相同结构的水平拆分表的逻辑名称。

比如我们将订单表`t_order` 分表拆分成 `t_order_0` ··· `t_order_9`等10张表，这时我们的数据库中已经不存在 `t_order`这张表，取而代之的是若干的`t_order_n`表。

分库分表通常对业务代码都是无侵入式的，开发者只专注于业务逻辑SQL编码，我们在代码中`SQL`依然按 `t_order`来写，而在执行逻辑SQL前将其解析成对应的数据库真实执行的SQL。此时 t_order 就是这些拆分表的`逻辑表`。

业务逻辑SQL

```sql
select * from t_order where order_no='A11111'
```

真实执行SQL

```sql
select * from DB_1.t_order_n where order_no='A11111'
```

## 真实表

真实表就是在数据库中真实存在的物理表`DB_1.t_order_n`。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397428.png)

## 广播表

广播表是一类特殊的表，其表结构和数据在所有分片数据源中均完全一致。与拆分表相比，广播表的数据量较小、更新频率较低，通常用于字典表或配置表等场景。由于其在所有节点上都有副本，因此可以大大降低`JOIN`关联查询的网络开销，提高查询效率。

需要注意的是，对于广播表的修改操作需要保证同步性，以确保所有节点上的数据保持一致。

**广播表的特点**：

- 在所有分片数据源中，广播表的数据完全一致。因此，对广播表的操作（如插入、更新和删除）会实时在每个分片数据源中执行一遍，以保证数据的一致性。
- 对于广播表的查询操作，仅需要在任意一个分片数据源中执行一次即可。
- 与任何其他表进行JOIN操作都是可行的，因为由于广播表的数据在所有节点上均一致，所以可以访问到任何一个节点上的相同数据。

> 什么样的表可以作为广播表呢？

订单管理系统中，往往需要查询统计某个城市地区的订单数据，这就会涉及到省份地区表`t_city`与订单流水表`DB_n`.`t_order_n`进行JOIN查询，因此可以考虑将省份地区表设计为`广播表`，核心理念就是**避免跨库JOIN操作**。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397529.png)

> **注意**：上边我们提到广播表在数据插入、更新与删除会实时在每个分片数据源均执行，也就是说如果你有1000个分片数据源，那么修改一次广播表就要执行1000次SQL，所以尽量不在并发环境下和业务高峰时进行，以免影响系统的性能。

## 单表

单表指所有的分片数据源中仅唯一存在的表（没有分片的表），适用于数据量不大且无需分片的表。

如果一张表的数据量预估在千万级别，且没有与其他拆分表进行关联查询的需求，建议将其设置为单表类型，存储在默认分片数据源中。

## 分片键

分片键决定了数据落地的位置，也就是数据将会被分配到哪个数据节点上存储。因此，分片键的选择非常重要。

比如我们将 `t_order` 表进行分片后，当插入一条订单数据执行SQL时，需要通过解析SQL语句中指定的分片键来计算数据应该落在哪个分片中。以表中`order_no`字段为例，我们可以通过对其取模运算（比如 `order_no % 2`）来得到分片编号，然后根据分片编号分配数据到对应的数据库实例（比如 `DB_1` 和 `DB_2`）。拆分表也是同理计算。

在这个过程中，`order_no` 就是 `t_order` 表的分片键。也就是说，每一条订单数据的 `order_no` 值决定了它应该存放的数据库实例和表。选择一个适合作为分片键的字段可以更好地利用水平分片带来的性能提升。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397530.png)

这样同一个订单的相关数据就会落在同一个数据库、表中，查询订单时同理计算，就可直接定位数据位置，大幅提升数据检索的性能，避免了全库表扫描。

不仅如此 `ShardingSphere` 还支持根据多个字段作为分片健进行分片，这个在后续对应章节中会详细讲。

## 分片策略

分片策略来指定使用哪种分片算法、选择哪个字段作为分片键以及如何将数据分配到不同的节点上。

分片策略是由`分片算法`和`分片健`组合而成，分片策略中可以使用多种分片算法和对多个分片键进行运算。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397531.png)

> 分库、分表的分片策略配置是相对独立的，可以各自使用不同的策略与算法，每种策略中可以是多个分片算法的组合，每个分片算法可以对多个分片健做逻辑判断。

## 分片算法

分片算法则是用于对分片键进行运算，将数据划分到具体的数据节点中。

常用的分片算法有很多：

- **哈希分片**：根据分片键的哈希值来决定数据应该落到哪个节点上。例如，根据用户 ID 进行哈希分片，将属于同一个用户的数据分配到同一个节点上，便于后续的查询操作。
- **范围分片**：分片键值按区间范围分配到不同的节点上。例如，根据订单创建时间或者地理位置来进行分片。
- **取模分片**：将分片键值对分片数取模，将结果作为数据应该分配到的节点编号。例如， order_no % 2 将订单数据分到两个节点之一。
- .....

实际业务开发中分片的逻辑要复杂的多，不同的算法适用于不同的场景和需求，需要根据实际情况进行选择和调整。

## 绑定表

绑定表是那些具有相同分片规则的一组分片表，由于分片规则一致所产生的的数据落地位置相同，在`JOIN`联合查询时能有效**避免跨库**操作。

比如：`t_order` 订单表和 `t_order_item` 订单项目表，都以 `order_no` 字段作为分片键，并且使用 `order_no` 进行关联，因此两张表互为绑定表关系。

> 使用绑定表进行多表关联查询时，必须使用分片键进行关联，否则会出现笛卡尔积关联或跨库关联，从而影响查询效率。

当使用 `t_order` 和 `t_order_item` 表进行多表联合查询，执行如下联合查询的逻辑SQL。

```
SELECT * FROM t_order o JOIN t_order_item i ON o.order_no=i.order_no
```

如果不配置绑定表关系，两个表的数据位置不确定就会全库表查询，出现笛卡尔积关联查询，将产生如下四条`SQL`。

```
SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_no=i.order_no 
SELECT * FROM t_order_0 o JOIN t_order_item_1 i ON o.order_no=i.order_no 
SELECT * FROM t_order_1 o JOIN t_order_item_0 i ON o.order_no=i.order_no 
SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_no=i.order_no 
```

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397532.png)

而配置绑定表关系后再进行关联查询时，分片规则一致产生的数据就会落到同一个库表中，那么只需在当前库中 `t_order_n` 和 `t_order_item_n` 表关联即可。

```
SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id 
SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id 
```

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397533.png)

> **注意**：在关联查询时 `t_order` 它作为整个联合查询的主表。所有相关的路由计算都只使用主表的策略，`t_order_item` 表的分片相关的计算也会使用 `t_order` 的条件，所以要保证绑定表之间的分片键要完全相同。

## SQL 解析

分库分表后在应用层面执行一条 SQL 语句时，通常需要经过以下六个步骤：`SQL 解析` -> `执⾏器优化` -> `SQL 路由` -> `SQL 改写` -> `SQL 执⾏` -> `结果归并` 。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397534.png)在这里插入图片描述

SQL解析过程分为`词法解析`和`语法解析`两步，比如下边查询用户订单的SQL，先用词法解析将这条SQL拆解成不可再分的原子单元。在根据不同数据库方言所提供的字典，将这些单元归类为关键字，表达式，变量或者操作符等类型。

```
SELECT order_no FROM t_order where  order_status > 0  and user_id = 10086 
```

接着语法解析会将拆分后的SQL关键字转换为抽象语法树，通过对抽象语法树遍历，提炼出分片所需的上下文，上下文包含查询字段信息（`Field`）、表信息（`Table`）、查询条件（`Condition`）、排序信息（`Order By`）、分组信息（`Group By`）以及分页信息（`Limit`）等，并标记出 SQL中有可能需要改写的位置。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397535.png)

## 执⾏器优化

执⾏器优化是根据SQL查询特点和执行统计信息，选择最优的查询计划并执行，比如`user_id`字段有索引，那么会调整两个查询条件的位置，主要是提高SQL的执行效率。

```
SELECT order_no FROM t_order where user_id = 10086 and order_status > 0
```

## SQL 路由

通过上边的SQL解析得到了分片上下文数据，在匹配用户配置的分片策略和算法，就可以运算生成路由路径，将 SQL 语句路由到相应的数据节点上。

简单点理解就是拿到分片策略中配置的分片键等信息，在从SQL解析结果中找到对应分片键字段的值，计算出 SQL该在哪个库的哪个表中执行，SQL路由又根据有无分片健分为 `分片路由` 和 `广播路由`。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397536.png)

> 有分⽚键的路由叫分片路由，细分为直接路由、标准路由和笛卡尔积路由这3种类型。

#### 标准路由

标准路由是最推荐也是最为常⽤的分⽚⽅式，它的适⽤范围是不包含关联查询或仅包含绑定表之间关联查询的SQL。

当 SQL分片健的运算符为 `=` 时，路由结果将落⼊单库（表），当分⽚运算符是` BETWEEN` 或` IN` 等范围时，路由结果则不⼀定落⼊唯⼀的库（表），因此⼀条逻辑SQL最终可能被拆分为多条⽤于执⾏的真实SQL。

```
SELECT * FROM t_order  where t_order_id in (1,2)
```

SQL路由处理后

```
SELECT * FROM t_order_0  where t_order_id in (1,2)
SELECT * FROM t_order_1  where t_order_id in (1,2)
```

#### 直接路由

直接路由是直接将SQL路由到指定⾄库、表的一种分⽚方式，而且直接路由可以⽤于分⽚键不在SQL中的场景，还可以执⾏包括⼦查询、⾃定义函数等复杂情况的任意SQL。

#### 笛卡尔积路由

笛卡尔路由是由⾮绑定表之间的关联查询产生的，比如订单表`t_order` 分片键是`t_order_id `和用户表`t_user`分片键是`t_order_id `，两个表的分片键不同，要做联表查询，会执行笛卡尔积路由，查询性能较低尽量避免走此路由模式。

```
SELECT * FROM t_order_0 t LEFT JOIN t_user_0 u ON u.user_id = t.user_id WHERE t.user_id = 1
SELECT * FROM t_order_0 t LEFT JOIN t_user_1 u ON u.user_id = t.user_id WHERE t.user_id = 1
SELECT * FROM t_order_1 t LEFT JOIN t_user_0 u ON u.user_id = t.user_id WHERE t.user_id = 1
SELECT * FROM t_order_1 t LEFT JOIN t_user_1 u ON u.user_id = t.user_id WHERE t.user_id = 1
```

> 无分⽚键的路由又叫做广播路由，可以划分为全库表路由、全库路由、 全实例路由、单播路由和阻断路由这 5种类型。

#### 全库表路由

全库表路由针对的是数据库 `DQL `和 `DML`，以及 `DDL`等操作，当我们执行一条逻辑表 `t_order` SQL时，在所有分片库中对应的真实表 `t_order_0` ···  `t_order_n` 内逐一执行。

#### 全库路由

全库路由主要是对数据库层面的操作，比如数据库 `SET` 类型的数据库管理命令，以及 TCL 这样的事务控制语句。

对逻辑库设置 `autocommit` 属性后，所有对应的真实库中都执行该命令。

```
SET autocommit=0;
```

#### 全实例路由

全实例路由是针对数据库实例的 DCL 操作（设置或更改数据库用户或角色权限），比如：创建一个用户 order ，这个命令将在所有的真实库实例中执行，以此确保 order 用户可以正常访问每一个数据库实例。

```
CREATE USER order@127.0.0.1 identified BY '程序员小富';
```

#### 单播路由

单播路由用来获取某一真实表信息，比如获得表的描述信息：

```
DESCRIBE t_order; 
```

`t_order` 的真实表是 `t_order_0` ···· `t_order_n`，他们的描述结构相完全同，我们只需在任意的真实表执行一次就可以。

#### 阻断路由

⽤来屏蔽SQL对数据库的操作，例如：

```
USE order_db;
```

这个命令不会在真实数据库中执⾏，因为 `ShardingSphere` 采⽤的是逻辑 Schema（数据库的组织和结构） ⽅式，所以无需将切换数据库的命令发送⾄真实数据库中。

## SQL 改写

SQL经过解析、优化、路由后已经明确分片具体的落地执行的位置，接着就要将基于逻辑表开发的SQL改写成可以在真实数据库中可以正确执行的语句。比如查询 `t_order` 订单表，我们实际开发中 SQL是按逻辑表 `t_order` 写的。

```
SELECT * FROM t_order
```

这时需要将分表配置中的逻辑表名称改写为路由之后所获取的真实表名称。

```
SELECT * FROM t_order_n
```

## SQL执⾏

将路由和改写后的真实 SQL 安全且高效发送到底层数据源执行。但这个过程并不能将 SQL 一股脑的通过 JDBC 直接发送至数据源执行，需平衡数据源连接创建以及内存占用所产生的消耗，它会自动化的平衡资源控制与执行效率。

## 结果归并

将从各个数据节点获取的多数据结果集，合并成一个大的结果集并正确的返回至请求客户端，称为结果归并。而我们SQL中的排序、分组、分页和聚合等语法，均是在归并后的结果集上进行操作的。

## 分布式主键

数据分⽚后，一个逻辑表（`t_order`）对应诸多的真实表（`t_order_n`），它们之间由于⽆法互相感知，主键ID都从初始值累加，所以必然会产⽣重复主键ID，此时主键不再唯一那么对于业务来说也就没意义了。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397537.png)

尽管可通过设置表⾃增主键 `初始值` 和 `步⻓` 的⽅式避免ID碰撞，但这样会使维护成本加大，可扩展性差。

这个时候就需要我们手动为一条数据记录，分配一个全局唯一的ID，这个ID被叫做分布式ID，而生产这个ID的系统通常被叫做发号器。

大家可以参考我之前发布的这篇文章 [9种分布式ID生成方案](https://mp.weixin.qq.com/s?__biz=MzAxNTM4NzAyNg==&mid=2247483785&idx=1&sn=8b828a8ae1701b810fe3969be536cb14&scene=21#wechat_redirect)

## 数据脱敏

分库分表数据脱敏是一种有效的数据保护措施，可以确保敏感数据的机密性和安全性，减少数据泄露的风险。

比如，我们在分库分表时可以指定表的哪些字段为脱敏列，并设置对应的脱敏算法，在数据分片时解析到执行SQL中有待脱敏字段，会直接将字段值脱敏后的写入库表内。

对于用户的个人信息，如姓名、地址和电话号码等，可以通过加密、随机化或替换成伪随机数据的方式进行脱敏，以确保用户的隐私得到保护。

大家可以参考我之前发布的这篇文章 [大厂也在用的 6种 数据脱敏方案](https://mp.weixin.qq.com/s?__biz=MzAxNTM4NzAyNg==&mid=2247489848&idx=1&sn=a4163a7c0914bd8e1ec1f29171ab7c92&scene=21#wechat_redirect)

## 分布式事务

分布式事务的核心问题是如何实现跨多个数据源的原子性操作。

由于不同的服务通常会使用不同的数据源来存储和管理数据，因此，跨数据源的操作可能会导致数据不一致性或丢失的风险。因此，保证分布式事务的一致性是非常重要的。

以订单系统为例，它需要调用支付系统、库存系统、积分系统等多个系统，而每个系统都维护自己的数据库实例，系统间通过API接口交换数据。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397538.png)

为了保证下单后多个系统同时调用成功，可以使用`强一致性事务`的XA协议，或者`柔性事务`的代表工具Seata，来实现分布式事务的一致性。这些工具可以帮助开发人员简化分布式事务的实现，减少错误和漏洞的出现，提高系统的稳定性和可靠性。

经过分库分表之后，问题的难度进一步提升。自身订单服务，也需要处理跨数据源的操作。这样一来，系统的复杂度显著增加。因此，不到万不得已的情况下，最好避免采用分库分表的解决方案。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397539.png)

关于分布式事务详细的介绍，大家可以参考我之前发布的这篇文章 [对比 5 种分布式事务方案，还是宠幸了阿里的 Seata（原理 + 实战）](https://mp.weixin.qq.com/s?__biz=MzAxNTM4NzAyNg==&mid=2247489120&idx=1&sn=3361751a1339ba80804c81ced0744b60&scene=21#wechat_redirect)

## 数据迁移

分库分表后还有个让人头疼的问题，那就是数据迁移，为了不影响现有的业务系统，通常会新建数据库集群迁移数据。将数据从旧集群的数据库、表迁移到新集群的分库、分表中。这是一个比较复杂的过程，在迁移过程中需要考虑`数据量`、`数据一致性`、`迁移速度`等诸多因素。

迁移主要针对 `存量数据` 和 `增量数据` 的处理，存量数据指旧数据源中已经存在且有价值的历史数据，增量数据指当下持续增长以及未来产生的业务数据。

存量数据可以采用定时、分批次的迁移，迁移过程可能会持续几天。

增量数据可以采用新、旧数据库集群双写模式。待数据迁移完毕，业务验证了数据一致性，应用直接切换数据源即可。

后续我们会结合三方工具，来演示迁移的过程。

## 影子库

什么是影子库（`Shadow Table`）？

影子库是一个与生产环境数据库结构完全相同的实例，它存在的意义是为了在不影响线上系统的情况下，验证数据库迁移或者其他数据库变更操作的正确性，以及全链路压测。影子库中存储的数据是从生产环境中定期复制过来的，但是它不对线上业务产生任何影响，仅用于测试，验证和调试。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918687397540.png)

在进行数据库升级、版本变更、参数调优等操作前，通过在影子库上模拟这些操作，可以发现潜在的问题，因为测试环境的数据是不可靠的。

在使用影子库时，需要遵循以下几个原则：

- 与生产环境数据库的结构应该完全一致，包括表结构、索引、约束等；
- 数据要与生产环境保持一致，可以通过定期同步方式实现；
- 读写操作不会影响生产环境，一般情况下应该禁止在影子库上执行更新、删除等操作；
- 由于影子库的数据特点，访问权限应该严格控制，只允许授权人员进行访问和操作；

# SpringBoot 实现分库分表

> 在使用`ShardingSphere`实现分库分表的时候，要摒弃**先建表、再配规则**的传统思维

## 一、常见的分库分表工具

市面上常见的分库分表工具，包括 `ShardingSphere`、`Cobar`、`Mycat`、`TDDL`、`MySQL Fabric` 等

### Cobar

Cobar 是阿里巴巴开源的一款基于MySQL的分布式数据库中间件，提供了分库分表、读写分离和事务管理等功能。它采用轮询算法和哈希算法来进行数据分片，支持分布式分表，但是不支持单库分多表。

它以 `Proxy` 方式提供服务，在阿里内部被广泛使用已开源，配置比较容易，无需依赖其他东西，只需要有Java环境即可。兼容市面上几乎所有的 ORM 框架，仅支持 MySQL 数据库，且事务支持方面比较麻烦。

### MyCAT

`Mycat` 是社区爱好者在阿里 Cobar 基础上进行二次开发的，也是一款比较经典的分库分表工具。它以 Proxy 方式提供服务，支持分库分表、读写分离、SQL路由、数据分片等功能。

兼容市面上几乎所有的 ORM 框架，包括 Hibernate、MyBatis和 JPA等都兼容，不过，美中不足的是它仅支持 MySQL数据库，目前社区的活跃度相对较低。

### TDDL

TDDL 是阿里巴巴集团开源的一款分库分表解决方案，可以自动将SQL路由到相应的库表上。它采用了垂直切分和水平切分两种方式来进行分表分库，并且支持多数据源和读写分离功能。

TDDL 是基于 Java 开发的，支持 MySQL、Oracle 和 SQL Server 数据库，并且可以与市面上 Hibernate、MyBatis等 ORM 框架集成。

不过，TDDL仅支持一些阿里巴巴内部的工具和框架的集成，对于外部公司来说可能相对有些局限性。同时，其文档和社区活跃度相比 ShardingSphere 来说稍显不足。

### Mysql Fabric

`MySQL Fabric`是 MySQL 官方提供的一款分库分表解决方案，同时也支持 MySQL其他功能，如高可用、负载均衡等。它采用了管理节点和代理节点的架构，其中管理节点负责实时管理分片信息，代理节点则负责接收并处理客户端的读写请求。

它仅支持 MySQL 数据库，并且可以与市面上 Hibernate、MyBatis 等 ORM 框架集成。MySQL Fabric 的文档相对来说比较简略，而且由于是官方提供的解决方案，其社区活跃度也相对较低。

### ShardingSphere

ShardingSphere 成员中的 sharding-jdbc 以 `JAR` 包的形式下提供分库分表、读写分离、分布式事务等功能，但仅支持 Java 应用，在应用扩展上存在局限性。

因此，ShardingSphere 推出了独立的中间件 sharding-proxy，它基于 MySQL协议实现了透明的分片和多数据源功能，支持各种语言和框架的应用程序使用，对接的应用程序几乎无需更改代码，分库分表配置可在代理服务中进行管理。

除了支持 MySQL，ShardingSphere还可以支持 PostgreSQL、SQLServer、Oracle等多种主流数据库，并且可以很好地与 Hibernate、MyBatis、JPA等 ORM 框架集成。重要的是，ShardingSphere的开源社区非常活跃。

如果在使用中出现问题，用户可以在 GitHub 上提交PR并得到快速响应和解决，这为用户提供了足够的安全感。

### 比较

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918736352556.png)

## 二、ShardingSphere 介绍

ShardingSphere 官网地址：https://shardingsphere.apache.org/

`shardingsphere` 是一款开源的分布式关系型数据库中间件，为 `Apache` 的顶级项目。其前身是 `sharding-jdbc` 和 `sharding-proxy` 的两个独立项目，后来在 2018 年合并成了一个项目，并正式更名为 ShardingSphere。

其中 sharding-jdbc 为整个生态中最为经典和成熟的框架，最早接触分库分表的人应该都知道它，是学习分库分表的最佳入门工具。

如今的 ShardingSphere 已经不再是单纯代指某个框架，而是一个完整的技术生态圈，由三款开源的分布式数据库中间件 sharding-jdbc、sharding-proxy 和 sharding-sidecar 所构成。

## 三、ShardingSphere 成员

ShardingSphere 的主要组成成员为`sharding-jdbc`、`sharding-proxy`，它们是实现分库分表的两种不同模式：

### sharding-jdbc

它是一款轻量级Java框架，提供了基于 JDBC 的分库分表功能，为客户端直连模式。

使用sharding-jdbc，开发者可以通过简单的配置实现数据的分片，同时无需修改原有的SQL语句。支持多种分片策略和算法，并且可以与各种主流的ORM框架无缝集成。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918736352657.png)

### sharding-proxy

它是基于 MySQL 协议的代理服务，提供了透明的分库分表功能。使用 sharding-proxy 开发者可以将分片逻辑从应用程序中解耦出来，无需修改应用代码就能实现分片功能，还支持多数据源和读写分离等高级特性，并且可以作为独立的服务运行。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170918736352658.png)

## 四、快速实现(shardingsphere-2fastcode)

我们先使用`sharding-jdbc`来快速实现分库分表。相比于 sharding-proxy，sharding-jdbc 适用于简单的应用场景，不需要额外的环境搭建等。下边主要基于 SpringBoot 的两种方式来实现分库分表，一种是通过`YML配置`方式，另一种则是通过纯`Java编码`方式（**不可并存**）。在后续章节中，我们会单独详细介绍如何使用`sharding-proxy`以及其它高级特性。

参考代码:  2-后端/note02/8-整合/综合/Springboot-Notebook/shardingsphere101/shardingsphere-2fastcode/README.md

<img src="01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/image-20240229161111675.png" alt="image-20240229161111675" style="zoom:80%;" />



# shardingsphere 自动化的表分片功能

**代码见: shardingsphere-autocreate-table**
<img src="01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/image-20240229161530085.png" alt="image-20240229161530085" style="zoom:80%;" />

`ShardingSphere`框架成员中的`Shardingsphere-jdbc`和`Shardingsphere-proxy`都提供了自动化管理分片表的功能`auto-tables`，可以统一维护大量的分片表，避免了手动编写脚本和维护分片表的繁琐工作，极大程度减少分库分表的开发和维护成本，提升效率和可靠性。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170919455706862.png)

## 思路

假设我们要对`t_order`表进行分库分表，首先我们要做的就是确定好分片方案，这里使用两个数据库实例`db0`、`db1`，每个实例中`t_order`表分成1000张分片表`t_order_1 ~ t_order_1000`，`order_id`字段作为分片键，分片算法使用取模算法`order_id % n`，分布式主键生成策略采用`snowflake`。

`t_order`逻辑表的表结构如下：

```sql
CREATE TABLE `t_order` (
 `order_id` BIGINT ( 20 ) NOT NULL COMMENT "订单表分布式主健ID",
 `order_number` VARCHAR ( 255 ) NOT NULL COMMENT "订单号",
 `customer_id` BIGINT ( 20 ) NOT NULL COMMENT "用户ID",
 `order_date` date NOT NULL COMMENT "下单时间",
 `total_amount` DECIMAL ( 10, 2 ) NOT NULL COMMENT "订单金额",
    PRIMARY KEY ( `order_id` ) USING BTREE 
);
```

## 分片规则配置

设定好分片规则，接着编写逻辑表`t_order`的分片规则的配置，分别使用`yml配置`和`Java编码`两种方式做了实现。**要注意的是两种方式不要并存，不然启动会报错**。

### yml配置方式

使用yml配置相对简单易用比较直观，适合对分库分表要求不太复杂的场景，完整配置如下：

```yaml
spring:
  shardingsphere:
    datasource:
      # 数据源名称，多数据源以逗号分隔 ,放在第一个的数据源为未配置分片规则表的默认数据源
      names: db0 , db1
      # 名称与上面 names 保持一致
      db0:
      ....

      db1:
      ....
    # 具体规则配置
    rules:
      sharding:
        # 分片算法定义
        sharding-algorithms:
          # 自定义分片算法名称
          t_order_database_algorithms:
            # 分片算法类型
            type: INLINE
            # 自定义参数
            props:
              algorithm-expression: db$->{order_id % 2}
          t_order_table_algorithms:
            type: INLINE
            props:
              algorithm-expression: t_order_$->{order_id % 1000}
          t_order_mod:
            type: MOD
            props:
              # 指定分片数量
              sharding-count: 1000
        # 分布式序列算法配置
        key-generators:
          t_order_snowflake:
            type: SNOWFLAKE
            # 分布式序列算法属性配置
            props:
              worker-id: 1
        tables:
          # 逻辑表名称
          t_order:
            # 数据节点：数据库.分片表
            actual-data-nodes: db$->{0..1}.t_order_$->{1..1000}
            # 分库策略
            database-strategy:
              standard:
                # 分片列名称
                sharding-column: order_id
                # 分片算法名称
                sharding-algorithm-name: t_order_database_algorithms
            # 分表策略
            table-strategy:
              standard:
                # 分片列名称
                sharding-column: order_id
                # 分片算法名称
                sharding-algorithm-name: t_order_table_algorithms
            # 主键生成策略
            keyGenerateStrategy:
              column: order_id
              keyGeneratorName: t_order_snowflake
    # 属性配置
    props:
      # 展示修改以后的sql语句
      sql-show: true
```

### Java编码方式

使用Java编码方式更加灵活和可扩展，可以根据业务定制分片规则，适合对分库分表有特殊需求或需要动态调整的场景。

```java
/**
 */
@Configuration
public class ShardingConfiguration {

    /**
     * 配置分片数据源
     */
    @Bean
    public DataSource getShardingDataSource() throws SQLException {
        Map<String, DataSource> dataSourceMap = new HashMap<>();
        dataSourceMap.put("db0", dataSource0());
        dataSourceMap.put("db1", dataSource1());

        // 分片rules规则配置
        ShardingRuleConfiguration shardingRuleConfig = new ShardingRuleConfiguration();

        // 分片算法
        shardingRuleConfig.setShardingAlgorithms(getShardingAlgorithms());
        // 配置 t_order 表分片规则
        ShardingTableRuleConfiguration orderTableRuleConfig = new ShardingTableRuleConfiguration("t_order", "db${0..1}.t_order_${1..1000}");
        orderTableRuleConfig.setTableShardingStrategy(new StandardShardingStrategyConfiguration("order_id", "t_order_table_algorithms"));
        orderTableRuleConfig.setDatabaseShardingStrategy(new StandardShardingStrategyConfiguration("order_id", "t_order_database_algorithms"));
        shardingRuleConfig.getTables().add(orderTableRuleConfig);

        // 是否在控制台输出解析改造后真实执行的 SQL
        Properties properties = new Properties();
        properties.setProperty("sql-show", "true");

        // 创建 ShardingSphere 数据源
        return ShardingSphereDataSourceFactory.createDataSource(dataSourceMap, Collections.singleton(shardingRuleConfig), properties);
    }

    /**
     * 配置数据源1
     */
    public DataSource dataSource0() {
        HikariDataSource dataSource = new HikariDataSource();
        dataSource.setDriverClassName("com.mysql.cj.jdbc.Driver");
        dataSource.setJdbcUrl("jdbc:mysql://127.0.0.1:3306/db0?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true");
        dataSource.setUsername("root");
        dataSource.setPassword("123456");
        return dataSource;
    }

    /**
     * 配置数据源2
     */
    public DataSource dataSource1() {
        HikariDataSource dataSource = new HikariDataSource();
        dataSource.setDriverClassName("com.mysql.cj.jdbc.Driver");
        dataSource.setJdbcUrl("jdbc:mysql://127.0.0.1:3306/db1?useUnicode=true&characterEncoding=utf-8&useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true");
        dataSource.setUsername("root");
        dataSource.setPassword("123456");
        return dataSource;
    }

    /**
     * 配置分片算法
     */
    private Map<String, AlgorithmConfiguration> getShardingAlgorithms() {
        Map<String, AlgorithmConfiguration> shardingAlgorithms = new LinkedHashMap<>();

        // 自定义分库算法
        Properties databaseAlgorithms = new Properties();
        databaseAlgorithms.setProperty("algorithm-expression", "db$->{order_id % 2}");
        shardingAlgorithms.put("t_order_database_algorithms", new AlgorithmConfiguration("INLINE", databaseAlgorithms));

        // 自定义分表算法
        Properties tableAlgorithms = new Properties();
        tableAlgorithms.setProperty("algorithm-expression", "db$->{order_id % 1000}");
        shardingAlgorithms.put("t_order_table_algorithms", new AlgorithmConfiguration("INLINE", tableAlgorithms));

        return shardingAlgorithms;
    }
}
```

上面我们在应用中编写好了分片规则，现在就差在数据库实例中创建分片表了，手动创建和管理1000张分片表确实是一个又脏又累的活，反正我是不会干的！

## 管理分片表

其实，`ShardingSphere`内已经为我们提供了管理分片表的能力。

当一张逻辑表`t_order`被配置了分片规则，那么接下来对逻辑表的各种`DDL`操作（例如`创建表`、`修改表结构`等），命令和数据会根据分片规则，执行和存储到每个分片数据库和分片库中的相应分片表中，以此保持整个分片环境的一致性。

不过，使用`Shardingsphere-jdbc`管理分片表的过程中，是需要我们手动编写对逻辑表的`DDL`操作的代码。

我们来跑几个单元测试用例来观察实际的执行效果，直接使用`jdbcTemplate`执行创建逻辑表`t_order`的SQL。

```java
package com.shardingsphere_101;

import org.junit.jupiter.api.DisplayName;
import org.junit.jupiter.api.Test;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.jdbc.core.JdbcTemplate;
import javax.annotation.Resource;

/**
 * 自动创建分片表
 */
@SpringBootTest
class AutoCreateTablesTests {

    @Resource
    private JdbcTemplate jdbcTemplate;

    /**
     * 执行创建逻辑表的SQL，会根据AutoTables的配置自动在对应的数据源内创建分片表
     *
     */
    @DisplayName("创建分片表")
    @Test
    public void autoCreateOrderTableTest() {

        jdbcTemplate.execute("CREATE TABLE `t_order` (\n" +
                "  `order_id` bigint(20) NOT NULL,\n" +
                "  `order_number` varchar(255) NOT NULL,\n" +
                "  `customer_id` bigint(20) NOT NULL,\n" +
                "  `order_date` date NOT NULL,\n" +
                "  `total_amount` decimal(10,2) NOT NULL,\n" +
                "  PRIMARY KEY (`order_id`) USING BTREE\n" +
                ");");
    }

    /**
     * 修改 t_order 表
     *
     */
    @DisplayName("修改分片表字段长度")
    @Test
    public void autoModifyOrderTableTest() {

        jdbcTemplate.execute("ALTER TABLE t_order MODIFY COLUMN order_number varchar(500);");
    }

    /**
     * 删除 t_order 表
     *
     */
    @DisplayName("删除分片表")
    @Test
    public void autoDeleteOrderTableTest() {

        jdbcTemplate.execute("DROP TABLE `t_order`;");
    }
}

```

根据之前配置的分片规则，将会在两个数据库实例 `db0` 和 `db1` 中，分别生成`1000`张命名为`t_order_1`到`t_order_1000`的分片表，看到两个数据库均成功创建了1000张分片表。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170919455706863.png)

在次执行更新`t_order`表SQL，将字段`order_number`长度从 `varchar(255)`扩展到 `varchar(500)`，执行SQL看下效果。

```java
@Test
public void autoModifyOrderTableTest() {
    jdbcTemplate.execute("ALTER TABLE t_order MODIFY COLUMN order_number varchar(500);");
}
```

通过查看两个分片库，我们成功地将所有分片表的`order_number`字段长度更改为了`varchar(500)`，在控制台日志中，可以看到它是通过在每个分片库内依次执行了1000次命令实现的。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170919455706964.png)

`Shardingsphere-jdbc`实现分库分表时，可以采用这种默认的方式来管理分片表。但要注意的是，由于涉及到不同的数据库实例，如果不使用第三方的分布式事务管理工具（例如`Seata`等），执行过程是无法保证事务一致性的。

## 自定义管理分片表

上边为逻辑表配置分片规则，应用程序内执行对逻辑表的DDL操作，就可以很轻松的管理分片表。

### 自定义

不过，默认的分片管理还是有局限性的，我们在设计分片规则时往往会根据不同的业务维度来划分，例如按天、月、按季度生成分片表并分布到不同数据源中等。这样就需要一些自定义的规则来实现。

`ShardingSphere 5.X`版本后推出了一种新的管理分片配置方式：`AutoTable`。设置了`AutoTable`的逻辑表，将交由`ShardingSphere`自动管理分片，用户只需要指定分片数量和使用的数据库实例，无需再关心表的具体分布，配置格式如下：

```
spring:
  shardingsphere:
    # 数据源配置
    datasource:
      ......
    # 具体规则配置
    rules:
      sharding:
        # 逻辑表分片规则
        tables:
          # 逻辑表名称
          t_order:
            .....
        # 自动分片表规则配置
        auto-tables:
          t_order: # 逻辑表名称
            actual-data-sources: db$->{0..1}
            sharding-strategy: # 切分策略
              standard: # 用于单分片键的标准分片场景
                sharding-column: order_id # 分片列名称
                sharding-algorithm-name: t_order_mod # 自动分片算法名称
```

`ShardingSphere-Jdbc`中配置使用`auto-tables`主要两个参数，`actual-data-sources`指定数据源分布，由于是管理分片表所以只需数据源信息即可；`sharding-strategy`指具体采用何种算法来进行分片。

> 对逻辑表的DDL操作，系统会首先检查是否配置了**AutoTable**，如果已配置，则优先采用配置的规则；若未配置，则将使用默认的逻辑表分片规则。

`AutoTable`支持`ShardingSphere`内置的全部自动分片算法，所谓自动分片算法就是根据`actualDataSources`设置的数据源信息，使用对应内置算法自行解析处理。

- MOD：取模分片算法
- HASH_MOD：哈希取模分片算法
- VOLUME_RANGE：基于分片容量的范围分片算法
- BOUNDARY_RANGE：基于分片边界的范围分片算法
- AUTO_INTERVAL：自动时间段分片算法

### AutoTable使用

举个例子，我们使用内置`MOD`取模算法作为`AutoTable`的分片算法，同样是`db0`、`db1`两个实例中各创建1000张分片表。那么当对逻辑表的DDL操作时，`ShardingSphere`会依据分片表编号`t_order_0～t_order_1999 % 数据库实例数`取模来确认DDL命令路由到哪个实例中执行。

```
spring:
  shardingsphere:
    # 数据源配置
    datasource:
      .....
    # 具体规则配置
    rules:
      sharding:
        # 自动分片表规则配置
        auto-tables:
          t_order:
            actual-data-sources: db$->{0..1}
            sharding-strategy:
              standard:
                sharding-column: order_date
                sharding-algorithm-name: t_order_mod
        # 分片算法定义
        sharding-algorithms:
          t_order_mod:
            type: MOD
            props:
              # 指定分片数量
              sharding-count: 2000
```

还是执行刚才创建表的单元测试，会发现`db0`、`db1`两个实例中已经各自创建了1000张分片表，但你会发现1000张表已经不再是按照顺序创建的了。

![图片](01-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%9A%84%E8%AF%B4%E6%98%8E.assets/640-170919455706965.png)

上边使用的是内置自动分片算法，它对于我们来说是黑盒，提供它方便我们拿来即用。不过，如果想要做到更细粒度的管理分片表，最好的办法就是自定义分片算法
