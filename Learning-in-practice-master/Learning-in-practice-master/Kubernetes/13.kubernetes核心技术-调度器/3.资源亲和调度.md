# Pod 资源亲和调度

Pod 对象间亲和性，将一些 Pod 对象组织在相近的位置(同一节点、机架、区域、地区)Pod 对象间反亲和性，将一些 Pod 在运行位置上隔开

### Pod 硬亲和调度

requiredDuringSchedulingIgnoredDuringExecution

Pod 亲和性描述一个 Pod 与具有某特征的现存 Pod 运行位置的依赖关系；即需要事先存在被依赖的 Pod 对象

```yaml
# 被依赖 Pod
kubectl run tomcat-1 app=tomcat--image tomcat:alpine
kubectl explain
pod.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution.to pologyKey

apiVersion:v1
kind:Pod
metadata:
  name:with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:# 硬亲和调度
      - labelSelector:
        matchExpressions:#集合选择器
        -{key:app,operator:In,values:["tomcat"]}# 选择被依赖 Pod
# 上面意思是，当前 pod 要跟标签为 app 值为 tomcat 的 pod 在一起
        topologyKey:kubernetes.io/hostname# 根据挑选出的 Pod 所有节点的 hostname
作为同一位置的判定
  containers:
  - name:myapp
    image:ikubernetes/myapp:v1
```

### Pod 软亲和调度

```yaml
apiVersion:apps/v1
kind:Deployment
metadata:
  name:myapp-with-preferred-pod-affinity
spec:
  replicas:3
  selector:
    matchLabels:
      app:myapp
  template:
    metadata:
      name:myapp
      labels:
        app:myapp
  spec:
    affinity:
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight:80
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              -{key:app,operator:In,values:["cache"]}
            topologyKey:zone
        - weight:20
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              -{key:app,operator:In,values:["db"]}
              topologyKey:zone
  containers:
  - name:myapp
    image:ikubernetes/myapp:v1
```

### Pod 反亲和调度

Pod 反亲和调度用于分散同一类应用，调度至不同的区域、机架或节点等将  spec.affinity.podAffinity 替换为 spec.affinity.podAntiAffinity

反亲和调度也分为柔性约束和强制约束

```yaml
apiVersion:v1
kind:Pod
metadata:
	name:pod-first
	labels:
		app:myapp
		tier:fronted
spec:
	containers:
	- name:myapp
	  image:ikubernetes/myapp:v1
---
apiVersion:v1
kind:Pod
metadata:
	name:pod-second
	labels:
		app:backend
		tier:db
spec:
	containers:
	- name:busybox
	  image:busybox:latest
	  imagePullPolicy:IfNotPresent
	  command:["/bin/sh","-c","sleep 3600"]
	affinity:
	  podAntiAffinity:
		requiredDuringSchedulingIgnoredDuringExecution:
		- labelSelector:
		  matchExpressions:
		  -{key:app,operator:In,values:["myapp"]}
		  topologyKey:zone
```

### 污点和容忍度

污点 taints 是定义在节点上的键值型属性数据，用于让节点拒绝将 Pod 调度运行于其上，除非 Pod 有接纳节点污点的容忍度容忍度 tolerations 是定义在 Pod 上的键值属性数据，用于配置可容忍的污点，且调度器将 Pod 调度至其能容忍该节点污点的节点上或没有污点的节点上

**使用 PodToleratesNodeTaints 预选策略和 TaintTolerationPriority 优选函数完成该机制**

节点亲和性使得 Pod 对象被吸引到一类特定的节点 (nodeSelector 和 affinity)

污点提供让节点排斥特定 Pod 对象的能力

- 定义污点和容忍度

  污点定义于 nodes.spec.taints 容忍度定义于 pods.spec.tolerations

  **语法： key=value:effect**

- effect 定义排斥等级

  NoSchedule，不能容忍，但仅影响调度过程，已调度上去的 pod 不受影响，仅对新增加的pod 生效。

  PreferNoSchedule，柔性约束，节点现存 Pod 不受影响，如果实在是没有符合的节点，也可以调度上来

  NoExecute，不能容忍，当污点变动时，Pod 对象会被驱逐

- 在 Pod 上定义容忍度时

  等值比较 容忍度与污点在 key、value、effect 三者完全匹配

  存在性判断 key、effect 完全匹配，value 使用空值

  一个节点可配置多个污点，一个 Pod 也可有多个容忍度

- 管理节点的污点

  同一个键值数据，effect 不同，也属于不同的污点

  给节点添加污点：

  ```yaml
  kubectl taint node <node-name><key>=<value>:<effect>
  kubectl taint node node2 node-type=production:NoShedule #举例
  ```

  查看节点污点：

  ```yaml
  kubectl get nodes <nodename> -o go-template={{.spec.taints}}
  ```

  删除节点污点：

  ```yaml
  kubectl taint node <node-name><key>[:<effect>]-
  kubectl patch nodes <node-name> -p '{"spec":{"taints":[]}}'
  kubectl taint node kube-node1 node-type=production:NoSchedule
  kubectl get nodes kube-node1 -o go-template={{.spec.taints}}
  ```

  删除 key 为 node-type，effect 为 NoSchedule 的污点
  
  ```yaml
  kubectl taint node kube-node1 node-type:NoSchedule-
  ```
  
  删除 key 为 node-type 的所有污点
  
  ```yaml
  kubectl taint node kube-node1 node-type-
  ```
  
  删除所有污点
  
  ```yaml
  kubectl patch nodes kube-node1 -p '{"spec":{"taints":[]}}'
  ```
  
  给 Pod 对象容忍度
  
  ```yaml
  spec.tolerations 字段添加
  tolerationSeconds 用于定义延迟驱逐 Pod 的时长
  ```
  
  等值判断
  
  ```yaml
  tolerations:
  - key:"key1"
    operator:"Equal"#判断条件为 Equal
    value:"value1"
    effect:"NoExecute"
    tolerationSeconds:3600
  ```
  
  存在性判断
  
  ```yaml
  tolerations:
  - key:"key1"
    operator:"Exists"#存在性判断，只要污点键存在，就可以匹配
    effect:"NoExecute"
    tolerationSeconds:3600
  
  
  apiVersion:v1
  kind:Deployment
  metadata:
    name:myapp-deploy
    namespace:default
  spec:
    replicas:3
    selector:
      matchLabels:
        pp:myapp
        release:canary
    template:
      metadata:
        labels:
          app:myapp
          release:canary
      spec:
        containers:
        - name:myapp
        image:ikubernetes/myapp:v1
        ports:
        - name:http
          containerPort:80
        tolerations:
        - key:"node-type"
          operator:"Equal"
          value:"production":
          effect:"NoExecute"
          tolerationSeconds:3600
  ```
  
### 问题节点标识

  自动为节点添加污点信息，使用 NoExecute 效用标识，会驱逐现有 Pod

  K8s 核心组件通常都容忍此类污点

  node.kubernetes.io/not-ready 节点进入 NotReady 状态时自动添加

  node.alpha.kubernetes.io/unreachable 节点进入 NotReachable 状态时自动添加

  node.kubernetes.io/out-of-disk 节点进入 OutOfDisk 状态时自动添加

  node.kubernetes.io/memory-pressure 节点内存资源面临压力

  node.kubernetes.io/disk-pressure 节点磁盘面临压力

  node.kubernetes.io/network-unavailable 节点网络不可用

  node.cloudprovider.kubernetes.io/uninitialized kubelet 由外部云环境程序启动时，自动添加，待到去控制器初始化此节点时再将其删除

### Pod 优选级和抢占式调度

优选级，Pod 对象的重要程度

优选级会影响节点上 Pod 的调度顺序和驱逐次序

一个 Pod 对象无法被调度时，调度器会尝试抢占(驱逐)较低优先级的 Pod 对象，以便可以调度当前 Pod

### Pod 优选级和抢占机制默认处于禁用状态

启用：同时为 kube-apiserver、kube-scheduler、kubelet 程序的 --feature-gates 添加PodPriority=true

使用：事先创建优先级类别，并在创建 Pod 资源时通过 priorityClassName 属性指定所属优选级类别




